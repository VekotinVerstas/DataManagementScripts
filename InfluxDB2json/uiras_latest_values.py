import argparse
import datetime
import json
import logging
import os
import sys
import tempfile

import pytz
from geojson import Feature, Point, FeatureCollection
from influxdb import InfluxDBClient

from uirasmeta import META


def usage():
    print(f"""python {sys.argv[0]} --host host.example.io --database dbname --measurement mname --field fname """)
    exit()


def get_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--log",
        choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
        default="ERROR",
        help="Set the logging level",
    )
    parser.add_argument("--database", help="Database name", required=True)
    parser.add_argument("--host", help="Database address (ip/url)", default="localhost", nargs="?")
    parser.add_argument("--port", help="Database port", type=int, default=8086, nargs="?")
    parser.add_argument("--username", help="DB user name", nargs="?")
    parser.add_argument("--password", help="DB password", nargs="?")
    parser.add_argument("--base_url", help="URL for all the data", nargs="?")
    parser.add_argument("--measurement", help="Measurement name", required=True)
    parser.add_argument("--field", help="Field name, e.g. 'batt'", required=True)
    parser.add_argument("--outfile", help="Output filename (default stdout)", nargs="?")
    parser.add_argument("--usage", action="store_true", help="Print usage text and exit")
    args = parser.parse_args()
    if args.usage:
        usage()
    logging.basicConfig(format="%(asctime)s %(levelname)-8s %(message)s", level=getattr(logging, args.log))
    return args


def sanitize_devid(devid: str) -> str:
    """
    Remove possible colons (:) from devid and return it in uppercase
    """
    return devid.replace(":", "").upper()


def get_data(args: argparse.Namespace) -> list:
    iclient = InfluxDBClient(host=args.host, port=args.port, database=args.database)
    devs = []
    query = f"""SELECT LAST({args.field}), * FROM {args.measurement} GROUP BY "dev-id" """
    result = iclient.query(query, epoch="ms")
    for p in result.items():
        devid: str = sanitize_devid(p[0][1]["dev-id"])
        data: dict = next(p[1])  # Result contains only one data line per device
        if data:
            ts = datetime.datetime.fromtimestamp(data["time"] / 1000, tz=datetime.timezone.utc)
            data.pop("last", None)  # remove "last" field generated by query
            data.update({"devid": devid, "time": ts})
            devs.append(data)
    return devs


def get_now() -> datetime.datetime:
    return pytz.UTC.localize(datetime.datetime.utcnow())


def to_geojson(uiras, base_url):
    devid = uiras["devid"]
    if devid not in META:
        return
    d = META[devid]
    if (get_now() - uiras["time"]).total_seconds() > 7 * 24 * 60 * 60:
        logging.warning(f"Discard more than 7 days old data: {d['name']}")
        return
    props = {
        "name": d["name"],
        "location": d.get("location", ""),
        "district": d.get("district", ""),
        "temp_water": uiras["temp_out1"],
        "temp_internal": uiras["temp_in"],
        "battery": uiras["batt"],
        "time": uiras["time"].isoformat(),
    }
    links = d.get("links", {})
    links.update(
        {
            "json": {
                "type": "application/json",
                "rel": "data",
                "title": "Data for 2 weeks in JSON format, version 1",
                "href": f"{base_url}{devid}_v1.json",
            }
        }
    )
    links.update(
        {
            "json v2": {
                "type": "application/json",
                "rel": "data",
                "title": "Data for 2 weeks in JSON format, version 2",
                "href": f"{base_url}{devid}_v2.json",
            }
        }
    )
    links.update(
        {
            "csv": {
                "type": "text/csv",
                "rel": "data",
                "title": "Data for 2 weeks in CSV format",
                "href": f"{base_url}{devid}.csv",
            }
        }
    )
    if d.get("servicemap_url", "") != "":
        links.update(
            {
                "servicemap": {
                    "type": "text/html",
                    "rel": "external",
                    "title": "Palvelukartta",
                    "href": d["servicemap_url"],
                },
            }
        )

    if d.get("site_url", "") != "":
        links.update(
            {
                "site": {
                    "type": "text/html",
                    "rel": "external",
                    "title": d.get("site_title", "Kotisivu"),
                    "href": d["site_url"],
                }
            }
        )
    feature = Feature(geometry=Point((d["lon"], d["lat"])), properties=props, links=links)
    return feature


def main():
    args = get_args()
    devs = get_data(args)
    srt = sorted(devs, key=lambda i: i["devid"])
    features = []
    base_url = args.base_url or ""
    for d in srt:
        feature = to_geojson(d, base_url)
        if feature is not None:
            features.append(feature)
    meta = {
        "created_at": get_now().isoformat(),
        "comment": "This is the 2nd version of UiRaS data, now in GeoJSON format. Use this instead of v1, please.",
        "contact": "Aapo Rista <aapo.rista@forumvirium.fi>",
    }
    feature_collection = FeatureCollection(features, meta=meta)
    json_data = json.dumps(feature_collection, indent=1)
    if args.outfile:
        try:
            with tempfile.NamedTemporaryFile(dir=os.path.dirname(args.outfile), delete=False) as fp:
                fp.write(json_data.encode())
            os.replace(fp.name, args.outfile)
        finally:
            try:
                os.unlink(fp.name)
            except OSError:
                pass
    else:
        print(json_data)


if __name__ == "__main__":
    main()
